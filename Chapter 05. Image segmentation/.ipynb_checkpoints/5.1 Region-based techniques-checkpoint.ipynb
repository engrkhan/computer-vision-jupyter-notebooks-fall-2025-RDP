{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Region-based techniques for image segmentation\n",
    "\n",
    "Image segmentation is the process of **assigning a label to every pixel in an image** such that pixels with the same label share certain characteristics. As a consequence, it produces regions whose pixels have similar properties, e.h. intensity, color, texture, location in the image. The result of image segmentation could be:\n",
    "- a set of segments that collectively cover the entire image (e.g. thresholding), \n",
    "- or a set of contours extracted from the image (e.g. edge detection).\n",
    "\n",
    "<img src=\"./images/image-segmentation-example.png\" width=\"550\"/>$\\\\[5pt]$\n",
    "\n",
    "Conceptually, two traditional approaches to image segmentation exist:\n",
    "- **Top-down segmentation**, which considers that pixels from the same object in the scene should be in the same segmented region.\n",
    "- **Bottom-up segmentation**, which establishes that similar pixels in the image must be in the same segmented region. $\\\\[5pt]$\n",
    "\n",
    "<center><img src=\"./images/bottom-up_top-down-segmentation.png\" width=\"500\"/>Image adopted from <a href=\"myfootnote1\">[1]</a></center>$\\\\[5pt]$\n",
    "\n",
    "We put the spotlight here on bottom-up segmentation approaches. Methods following such approach can be grouped into:\n",
    "- **Contour-based techniques**, which attemp to identify the image regions by detecting their contours.\n",
    "- **Region-based techniques** that group together pixels that are similar. \n",
    "\n",
    "In this book, we will cover both families of techniques, starting with two popular region-based methods: \n",
    "\n",
    "- K-means (section 5.1.1)\n",
    "- Expectation-Maximization (EM, section 5.1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context - Color quantization\n",
    "\n",
    "<img src=\"./images/color-quantization.jpg\" width=\"800\"/>$\\\\[5pt]$\n",
    "\n",
    "Color quantization is the process of reducing the number of distinct colors in an image while preserving its color appearance as much as possible. It has many applications, like image compression (e.g. GIFs) or [content-based image retrieval](https://en.wikipedia.org/wiki/Content-based_image_retrieval). \n",
    "\n",
    "Image segmentation techniques can be used to achieve color quantization, let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "images_path = './images/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.PlotEllipse import PlotEllipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 K-Means\n",
    "\n",
    "As commented, region-based techniques try to group together pixels that are similar. Such problem is often called the *clustering problem*. Different attributes can be used to decide if two pixels are similar or not: intensity, texture, color, pixel location, etc.\n",
    "\n",
    "The **k-means algorithm** is a region-based technique that, given a set of elements, makes $K$ clusters out of them. Thereby, it is a perfect technique for addressing color quantization, since our goal is to reduce the color palette of an image to a fixed number of colors $K$.\n",
    "\n",
    "But, **how k-means algorithm works** in a color domain, where each pixel is represented in a n-dimensional space? (e.g. grayscale images define a 1D space, while RGB images a 3D space):\n",
    "\n",
    "1. Pick the number $K$, that is, the number of clusters in which image will be segmented (e.g. number of colors).\n",
    "2. Place $K$ centroids in the color space (e.g. randomly), these are the centers of the regions. \n",
    "3. Each pixel is assigned to the cluster with the closest centroid, hence creating new clusters. \n",
    "\n",
    "<img src=\"./images/kmeans-step-1.png\" width=\"400\"/><center><i>Example in a 2D space (e.g. YCbCr color space) with 3 clusters. Each point is assigned to its closer centroid</i></center>$\\\\[5pt]$\n",
    "\n",
    "4. Compute the new means of the clusters. \n",
    "\n",
    "<img src=\"./images/kmeans-step-2.png\" width=\"400\" align=\"center\"/><center><i>Example of how the centroids evolve over time</i></center>$\\\\[5pt]$\n",
    "\n",
    "5. Repeat steps 3 and 4 until convergence, that is, some previously defined criteria is fulfilled.$\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/kmeans-step-3.png\" width=\"400\" align=\"center\"/><center><i>Final segmentation result</i></center>$\\\\[5pt]$\n",
    "\n",
    "The procedure is the same independently of the number of dimensions in the workspace. \n",
    "\n",
    "This technique presents a number of pros and cons:\n",
    "\n",
    "- **Pros:**\n",
    "  - It's simple.\n",
    "  - Convergence to a local minima is guaranteed (but no guarantee to reach the global minima).\n",
    "- **Cons:**\n",
    "  - High usage of memory.\n",
    "  - The K must be fixed.\n",
    "  - Sensible to the selection of the initialization (initial position of centroids).\n",
    "  - Sensible to outliers.\n",
    "  - Circular clusters in the feature space are assumed (because of the usage of the Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means toy example\n",
    "\n",
    "Luckily for us, OpenCV defines a method that perform k-means: [`cv2.kmeans()`](https://docs.opencv.org/2.4/modules/core/doc/clustering.html). Let's take a look at a toy 1D k-means example in order to get familiar with it. The following function, `binarize_kmeans()`, binarizes an input `image` by executing the K-means algorithm, where the `it` sets its maximum number of iterations (it represents stopping/convergence criteria). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_kmeans(image,it):\n",
    "    \"\"\" Binarize an image using k-means.   \n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            it: K-means iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Set random seed for centroids \n",
    "    cv2.setRNGSeed(124)\n",
    "    \n",
    "    # Flatten image\n",
    "    flattened_img = image.reshape((-1,1))\n",
    "    flattened_img = np.float32(flattened_img)\n",
    "    \n",
    "    #Set epsilon\n",
    "    epsilon = 0.2\n",
    "    \n",
    "    # Estabish stopping criteria (either `it` iterations or moving less than `epsilon`)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, it, epsilon)\n",
    "    \n",
    "    # Set K parameter (2 for thresholding)\n",
    "    K = 2\n",
    "    \n",
    "    # Call kmeans using random initial position for centroids\n",
    "    _,label,center=cv2.kmeans(flattened_img,K,None,criteria,it,cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Colour resultant labels\n",
    "    center = np.uint8(center)\n",
    "    flattened_img = center[label.flatten()]\n",
    "    \n",
    "    # Reshape vector image to original shape\n",
    "    binarized = flattened_img.reshape((image.shape))\n",
    "    \n",
    "    # Show resultant image\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Original image\")\n",
    "    plt.imshow(binarized, cmap='gray',vmin=0,vmax=255)\n",
    "    \n",
    "    # Show how original histogram have been segmented\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title(\"Segmented histogram\")\n",
    "    plt.hist([image[binarized==center[0]].ravel(), image[binarized==center[1]].ravel()],256,[0,256], color=[\"black\",\"gray\"],stacked=\"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stopping criteria can be either if a maximum number of iterations is reached, or if the centroid moved less than a certain **epsion** value in a iteration.\n",
    "\n",
    "As you can see, [`cv2.kmeans()`](https://docs.opencv.org/2.4/modules/core/doc/clustering.html) returns two relevant arguments:\n",
    "\n",
    "- label: Integer array that stores the cluster index for every sample.\n",
    "- center: Matrix containing the cluster centroids (each row represents a different centroid)\n",
    "\n",
    "It is provided below an interactive code so you can play with `cv2.kmeans()` by calling it with different `it`values. After trying it **you are asked to explain** what `cv2.kmeans()` is doing in each iteration.\n",
    "\n",
    "*As you can see, if k=2 in a grayscale image, it is a binarization method that doesn't need to fix a manual threshold. We could have used it, for example, when dealing with the plate recognition problem!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "image = cv2.imread(images_path + 'plate.jpg',0)\n",
    "\n",
    "interact(binarize_kmeans, image=fixed(image),it=(2,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for 1D spaces and not high-resolution images k-means is very fast! (it only needs a few iterations to converge). What happens if k-means is applied with color images (3D space) in order to get color quantization?  \n",
    "\n",
    "Now that you know how k-means works, you can experimentally answer such question!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Playing with K-means</i></b></span>**\n",
    "\n",
    "Write an script that:\n",
    "- applies k-means to `malaga.png` with different values for $K$: $K=4$, $K=8$ and $K=16$,\n",
    "- shows, in a $2\\times2$ subplot, the 3 resulting images along with the input one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 12.0)\n",
    "\n",
    "# Read RGB image\n",
    "\n",
    "\n",
    "# Flatten image\n",
    "\n",
    "# Set criteria\n",
    "\n",
    "\n",
    "# Apply k-means\n",
    "\n",
    "\n",
    "# Colour resultant labels\n",
    "\n",
    "\n",
    "# Reshape to original shape\n",
    "\n",
    "\n",
    "# Show original image\n",
    "\n",
    "\n",
    "# Show k=4\n",
    "\n",
    "\n",
    "# Show k=8\n",
    "\n",
    "\n",
    "# Show k=16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Now, **answer the following questions**:\n",
    "\n",
    "- What number of maximum iterations did you use? Why?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- How could it be possible to compress these images? *Note: consider that a pixel needs 3 bytes to be represented, 8 bits per band.*\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 0px 5px; color:blue\"><i>Your answer here!</i></p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **<span style=\"color:green\"><b><i>EXTRA ASSIGNMENT: Analyzing execution times</i></b></span>**\n",
    "\n",
    "In this exercise you are asked to compare the execution time of K-means in a grayscale image, with K-means in a RGB image. Use the image `malaga.png` for this task, and use the same number of clusters and criteria for both, the grayscale and the RGB images.\n",
    "\n",
    "*Tip: [how to measure execution time in Python](https://stackoverflow.com/questions/14452145/how-to-measure-time-taken-between-lines-of-code-in-python)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Measuring the execution time needed for ...\")\n",
    "\n",
    "\n",
    "# Read images\n",
    "\n",
    "\n",
    "# Set criteria\n",
    "\n",
    "\n",
    "# Start timer\n",
    "\n",
    "# Flatten image\n",
    "\n",
    "\n",
    "# Apply k-means\n",
    "\n",
    "\n",
    "# Stop timer\n",
    "\n",
    "# Start timer\n",
    "\n",
    "# Flatten image\n",
    "\n",
    "\n",
    "# Apply k-means\n",
    "\n",
    "\n",
    "# Stop timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Expectation-Maximization (EM)\n",
    "\n",
    "**Expectation-Maximization (EM)** is the generalization of the K-means algorithm, where each cluster is represented by a Gaussian distribution, parametrized by a mean and a covariance matrix, instead of just a centroid. It's a *soft clustering* since it doesn't give *hard* decisions where a pixel belongs or not to a cluster, but the probability of that pixel belonging to each cluster $C_j$, that is, $p(x|C_j) \\sim N(\\mu_j,\\Sigma_j)$. This implies that at each algorithm iteration not just the mean of each cluster is refined (as in K-means), but also their covariance matrices.\n",
    "\n",
    "Before going into detail on the theory behind EM, it is worth seeing how it performs in the car plate problem. OpenCV provides a class implementing the needed functionality for applying EM segmentation to an image, called [cv2.ml.EM](https://docs.opencv.org/3.4/d1/dfb/classcv_1_1ml_1_1EM.html). All methods and parameters are fully detailed in the documentation, so it is a good idea to take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "cv2.setRNGSeed(5)\n",
    "\n",
    "# Define parameters\n",
    "n_clusters = 2 \n",
    "covariance_type = 0 # 0: covariance matrix spherical. 1: covariance matrix diagonal. 2: covariance matrix generic\n",
    "n_iter = 10\n",
    "epsilon = 0.2\n",
    "\n",
    "# Create EM empty object\n",
    "em = cv2.ml.EM_create()\n",
    "\n",
    "# Set parameters\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, n_iter, epsilon)\n",
    "em.setClustersNumber(n_clusters)\n",
    "em.setCovarianceMatrixType(covariance_type)\n",
    "em.setTermCriteria(criteria)\n",
    "\n",
    "# Read grayscale image\n",
    "image = cv2.imread(images_path + \"plate.jpg\",0)\n",
    "\n",
    "# Flatten image\n",
    "flattened_img = image.reshape((-1,1))\n",
    "flattened_img = np.float32(flattened_img)\n",
    "\n",
    "# Apply EM\n",
    "_, _, labels, _ = em.trainEM(flattened_img)\n",
    "\n",
    "# Reshape labels to image size (binarization)\n",
    "binarized = labels.reshape((image.shape))\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"Binarized image\")\n",
    "plt.imshow(binarized, cmap=\"gray\")\n",
    "\n",
    "# --------------- Gaussian visualization ---------------\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"Probabilities of the clusters\")\n",
    "\n",
    "# Get means and covs (for grayscale 1D both)\n",
    "means = em.getMeans()\n",
    "covs = em.getCovs()\n",
    "\n",
    "# Get standard deviation as numPy array\n",
    "sigmas = np.sqrt(covs)\n",
    "sigmas = sigmas[:,0,0]\n",
    "\n",
    "# Cast list to numPy array\n",
    "means = np.array(means)[:,0]\n",
    "\n",
    "# Plot Gaussians\n",
    "x = np.linspace(0, 256, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, loc = means[0], scale = sigmas[0]))\n",
    "plt.plot(x, stats.norm.pdf(x, loc = means[1], scale = sigmas[1]))\n",
    "plt.legend(['Black Region', 'White Region'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, although in OpenCV k-means is implemented as a method and EM as a class, they operate in a similar way. In the example above, we are segmenting a car plate into two clusters, and **each cluster is defined by a Gaussian distribution** (a Gaussian distribution for the black region, and another one for the white region). This is the basis of EM, **but how it works**?  \n",
    "\n",
    "EM is an iterative algorithm that is divided into two main steps:\n",
    "\n",
    "- First of all, it **initializes the mean and covariance matrix of each of the $K$ clusters**. Typically, it picks at random ($\\mu_j$,$\\Sigma_j$) and $P(C_j)$ (prior probability) for each cluster $j$.\n",
    "- Then, it keeps iterating doing Expectation-Maximization steps until some stopping criteria is satisfied (e.g. when no change occurs in a complete iteration):$\\\\[1pt]$\n",
    "    1. **Expectation step:** calcule the probabilities of every point belonging to each cluster, that is $p(C_j|x_i), \\forall i \\in data$:\n",
    "    $$P(C_j|x_i)=\\frac{p(x_i|C_j)p(C_j)}{p(x_i)}=\\frac{p(x_i|C_j)p(C_j)}{\\sum_i P(x_i|C_j)p(C_j)}$$\n",
    "    assign $x_i$ to the cluster $C_j$ with the highest probability $P(C_j|x_i)$.$\\\\[10pt]$\n",
    "    2. **Maximization step:** re-estimate the cluster parameters (($\\mu_j$,$\\Sigma_j$) and $p(C_j)$) for each cluster $j$ knowing the expectation step results, which is also called *Maximum Likelihood Estimate* (MLE):\n",
    "    $$\\mu_j=\\frac{\\sum_i p(C_j|x_i)x_i}{\\sum_i p(C_j|x_i)}\\\\\n",
    "    \\sum_j = \\frac{\\sum_i p(C_j|x_i)(x_i-\\mu_j)(x_i-\\mu_j)^T}{\\sum_i p(C_j|x_i)}\\\\\n",
    "    p(C_j)=\\sum_i p(C_j|x_i)p(x_i)=\\frac{\\sum_i p(C_j|x_i)}{N}$$\n",
    "    *Note that if no other information is available, the priors are considered equally probable.*\n",
    "    \n",
    "<img src=\"./images/em.gif\" width=\"400\" align=\"center\"><center>*Example of an execution of the EM algorithm with two clusters, with details about the evolution of their associated Gaussian distributions.*</center>$\\\\[5pt]$\n",
    "\n",
    "\n",
    "Doesn't it remind you to the K-means algorithm? **What is the difference between them?** \n",
    "\n",
    "The main difference is that K-means employs the **euclidian distance** to measure how near is a point to a cluster. In EM we use a distance in which **each dimension is weighted** by the **covariance matrix** of each cluster, which is also called **Mahalanobis distance**. Furthermore, for k-means a point of data **belongs or not to** a cluster, in EM a point of data have a higher or lower **probability** to belong to a cluster. The table below summarizes other differences:\n",
    "\n",
    "<table width=\"500px\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"></td>\n",
    "        <td style=\"text-align:center;\"><b>K-means</b></td>\n",
    "        <td style=\"text-align:center;\"><b>EM</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>Cluster representation</b></td>\n",
    "        <td style=\"text-align:center;\">Mean</td>\n",
    "        <td style=\"text-align:center;\">Mean, (co)variance</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>Cluster initialization</b></td>\n",
    "        <td style=\"text-align:center;\">Randomly select K means</td>\n",
    "        <td style=\"text-align:center;\">Initialize K Gaussian <br />distributions ($\\mu_j$,$\\Sigma_j$) and $P(C_j)$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>Expectation:</b> <br />\n",
    "Estimate the cluster of each data</td>\n",
    "        <td style=\"text-align:center;\">Assign each point to the closest mean </td>\n",
    "        <td style=\"text-align:center;\">Compute $P(C_j|x_i)$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>Maximization:</b> <br /> Re-estimate the cluster parameters</td>\n",
    "        <td style=\"text-align:center;\">Compute means of current clusters</td>\n",
    "        <td style=\"text-align:center;\">Compute new ($\\mu_j,\\Sigma_j$), $P(C_j)$ for each cluster $j$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "If you still curious about EM, you can find [here](https://www.youtube.com/watch?v=REypj2sy_5U) a more detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">OpenCV pill</font>\n",
    "\n",
    "Going back to code, working with EM we have to specify a covariance matrix type using [`em.setCovarianceMatrixType()`](https://docs.opencv.org/3.4/d1/dfb/classcv_1_1ml_1_1EM.html#a8b383c62697eac9a972931674790f6cd). Also, when you applying [`em.trainEM()`](https://docs.opencv.org/3.4/d1/dfb/classcv_1_1ml_1_1EM.html#a5a6a7badbc0c85a8c9fa50a41bf1bcd2) it doesn't return the centroid of the clusters, it is possible to get them calling [`em.getMeans()`](https://docs.opencv.org/3.4/d1/dfb/classcv_1_1ml_1_1EM.html#acec62dd55c06711c81d741c2d96603d1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Color quantization with YCrCb color space</i></b></span>**\n",
    "\n",
    "In the next example, color quantization is realized using the YCrCb color space (more info about such a space in appendix 2) instead of RGB. In this way, color quantization is only applied to the two color bands, neglecting th grayscale one. Let's see how it performs!\n",
    "\n",
    "**What to do?** Understand and test the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "cv2.setRNGSeed(5)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "n_clusters = 3 # Don't modify this parameter for this exercise\n",
    "\n",
    "covariance_type = # 0: Spherical covariance matrix. 1: Diagonal covariance matrix. 2: Full covariance matrix\n",
    "n_iter = 10\n",
    "epsilon = 0.2\n",
    "\n",
    "# Create EM empty object\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "\n",
    "# Read color image\n",
    "\n",
    "\n",
    "# Convert to YCrCb\n",
    "\n",
    "\n",
    "# Take color bands (2 lasts)\n",
    "\n",
    "\n",
    "# Flatten image\n",
    "\n",
    "\n",
    "# Apply EM\n",
    "\n",
    "\n",
    "# Colour resultant labels\n",
    "\n",
    "\n",
    "# Reshape to original shape\n",
    "\n",
    "\n",
    "# Merge original first band with quantized color bands\n",
    "\n",
    "\n",
    "# Cast to unsigned data dype\n",
    "\n",
    "\n",
    "# Reconvert to RGB\n",
    "\n",
    "\n",
    "# Show original image\n",
    "\n",
    "\n",
    "# Show resultant image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (2)</i></b></font>\n",
    "\n",
    "Once you understanded the code above, **answer the following questions:**\n",
    "\n",
    "- Why are the obtained results so good using only 3 clusters?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- What compression would be better in terms of space in memory, a 16-color compression in a RGB image or a 4-color compression in a YCrCb image?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 0px 5px; color:blue\"><i>Your answer here!</i></p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving deeper into covariance matrices\n",
    "\n",
    "There are 3 types of covariance matrices: **spherical covariances**, **diagonal covariances** or **full covariances**: $\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/ellipses.png\" width=\"700\" align=\"center\">$\\\\[10pt]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Visualizing clusters from EM</i></b></span>**\n",
    "\n",
    "Next, you have a code for visualizing the clusters in the YCrCb color space using EM.  \n",
    "\n",
    "**What to do?** Run the previous example modifying the type of covariance in the EM algorithm and visualize the changes using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "# Get means (2D) and covariance matrices (2x2)\n",
    "means = np.array(em.getMeans())\n",
    "covs = np.array(em.getCovs())\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([16, 240, 16, 240])\n",
    "\n",
    "# Get points contained in each cluster\n",
    "cluster_1 = np.any(color_bands == np.unique(res,axis=0)[0,:],axis=2)\n",
    "cluster_2 = np.any(color_bands == np.unique(res,axis=0)[1,:],axis=2)\n",
    "cluster_3 = np.any(color_bands == np.unique(res,axis=0)[2,:],axis=2)\n",
    "cluster_1 = image[cluster_1]\n",
    "cluster_2 = image[cluster_2]\n",
    "cluster_3 = image[cluster_3]\n",
    "\n",
    "# Plot them\n",
    "plt.plot(cluster_1[:,1],cluster_1[:,2],'go')\n",
    "plt.plot(cluster_2[:,1],cluster_2[:,2],'ro')\n",
    "plt.plot(cluster_3[:,1],cluster_3[:,2],'bo')\n",
    "\n",
    "# Plot ellipses representing covariance matrices\n",
    "PlotEllipse(fig, ax, np.vstack(means[0,:]), covs[0,:,:], 2, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(means[1,:]), covs[1,:,:], 2, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(means[2,:]), covs[2,:,:], 2, color='black')\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (3)</i></b></font>\n",
    "\n",
    "**Answer the following questions** about how clustering works in EM:\n",
    "\n",
    "- What are the differences between each type of covariance?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- What type of covariance makes EM equivalent to k-means?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 0px 5px; color:blue\"><i>Your answer here!</i></p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Applying EM considering different color spaces</i></b></span>**\n",
    "\n",
    "It's time to show what have you learned about **EM** and **color spaces**!\n",
    "\n",
    "**What is your task?** You are asked to **compare color quantization in a RGB color space and in a YCrCb color space**.\n",
    "\n",
    "For that:\n",
    "- apply Expectation-Maximization to `malaga.png` using 4 clusters (colors) to both the RGB-space image and the YCrCb-space one,\n",
    "- and display both results along with the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "<img src=\"images/exercise2.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 4\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 12.0)\n",
    "cv2.setRNGSeed(5)\n",
    "\n",
    "# Define parameters\n",
    "n_clusters = 4\n",
    "covariance_type = 2 # 0: covariance matrix spherical. 1: covariance matrix diagonal. 2: covariance matrix generic\n",
    "n_iter = 10\n",
    "epsilon = 0.2\n",
    "\n",
    "# Create EM empty objects\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "\n",
    "# Read image\n",
    "\n",
    "\n",
    "# Convert image to RGB\n",
    "\n",
    "\n",
    "# Convert image to YCrCb\n",
    "\n",
    "\n",
    "# Flatten RGB image\n",
    "\n",
    "\n",
    "# Flatten color bands of YCrCb image\n",
    "\n",
    "\n",
    "# Apply EM and get centers of clusters\n",
    "\n",
    "\n",
    "# Colour resultant labels\n",
    "\n",
    "\n",
    "# Reshape to original shape\n",
    "\n",
    "\n",
    "# Merge original first band with quantized color bands for YCrCb image\n",
    "\n",
    "\n",
    "# Cast YCrCb image to unsigned data dype\n",
    "\n",
    "\n",
    "# Reconvert YCrCb image back to RGB\n",
    "\n",
    "\n",
    "# Show original image\n",
    "\n",
    "\n",
    "# Show resultant quantization using RGB color space\n",
    "\n",
    "\n",
    "# Show resultant quantization using YCrCb color space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations for getting this work done! You have learned:\n",
    "\n",
    "- how k-means clustering works and how to use it,\n",
    "- how EM algorithm performs and how to employ it,\n",
    "- how to carry out color quantization and the importance of color spaces in this context, and\n",
    "- some basics for image compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "You have used YCrCb in this notebook because you were already fimiliar with it. The truth is that, in color quantization matters, [Lab color space](https://en.wikipedia.org/wiki/CIELAB_color_space) is commonly used.  \n",
    "\n",
    "Surf the internet for information about the Lab color space and then **answer the following questions**:\n",
    "\n",
    "- How does Lab color space work?\n",
    "- Why is it typically used for color quantization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a name=\"myfootnote1\">[1]</a>: Borenstein, Eran, Eitan Sharon, and Shimon Ullman. [Combining top-down and bottom-up segmentation.](http://www.wisdom.weizmann.ac.il/~vision/courses/2006_2/papers/recog_seg/Borenstein%20combining%20top-down%20and%20bottom-up%20segmentation.pdf). IEEE Conference on Conference on Computer Vision and Pattern Recognition Workshop, 2004."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "855.85px",
    "left": "1494px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
