{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Normal distribution naive Bayes classifier\n",
    "\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na√Øve) independence assumptions between the features:$\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/bayes.png\" width=\"700\">\n",
    "\n",
    "A **normal distribution naive Bayes classifier** supposes that input parameters follow the probability density function of a normal distribution:$\\\\[5pt]$\n",
    "\n",
    "\n",
    "$$p(\\mathbf{x}/C_i) = \\large{\\frac{1}{(2\\pi)^{n/2}|\\Sigma^i|^{1/2}}}e^{-(\\mathbf{x}-\\mu^i)^T\\Sigma_i^{-1}\\mathbf{x}-\\mu^i)}\\\\[5pt]$$\n",
    "\n",
    "In this notebook, will explain how Bayesian classifiers work (section 7.2.1) and, specifically, how to classify a feature vector supposing that follows a normal distribution (section 7.2.2).\n",
    "\n",
    "*Naive Bayes classifier is a Bayesian classifier where independence between all variables is assumed. In this chapter we will work only with naive classifiers.*\n",
    "\n",
    "## Problem context - Traffic sign recognition\n",
    "\n",
    "In the previous notebook, *AliquindoiCars* contacted us for a TSR technique to be integrated into a self-driving car. They provided us with some segmented images of traffic signs that their autonomous cars captured during test drivings. These images are located in `images/circles/` containing circled signs, `images/triangles` containing signs having triangle shapes and `images/squares` containing signs having square shapes.$\\\\[3pt]$\n",
    "\n",
    "<img src=\"./images/signs.jpg\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "Previously, we extracted a feature vector for each data image using Hu moments. Now, we will train a classifier using the feature vectors we saved in previous notebook. For classification, there are many methods we can apply, such as [kNN algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), [random forest](https://en.wikipedia.org/wiki/Random_forest), ... In this notebook, we will learn and use a naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "\n",
    "images_path = './images/'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.PlotEllipse import PlotEllipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Bayesian classifier\n",
    "\n",
    "Having a set of data variables $\\mathbf{x} = [x_1, x_2, x_3, \\ldots, x_n]^T$ and a set of possible classes $\\mathbf{C} = [C_1, C_2, C_3, \\ldots, C_n]$. The bayesian classifier assigns $\\mathbf{x}$ to the class $C_i$ that has the highest posterior probability $P(C_i/x)$ (the more probable, the less probability of making a mistake). This is called a **MAP** prediction:$\\\\[3pt]$\n",
    "\n",
    "<img src=\"./images/MAP.png\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "In this example, $\\mathbf{x}$ have only one variable. Deppending on the value of the variable, $\\mathbf{x}$ (feature vector of some object) is assigned to a class or not. Sometimes, it is convenient to have a *rejection region*, where no decision is made (e.g. $P(C_1/x) = P(C_2/x) = 0.5$).$\\\\[3pt]$\n",
    "\n",
    "<img src=\"./images/rejectregion.png\" width=\"300\">$\\\\[3pt]$\n",
    "\n",
    "**Discriminant function**\n",
    "\n",
    "We can obtain a discriminant function $d_i(x)$ for each class $C_i$, such that $d_i(x) \\gt d_j(x)$ whenever $P(C_i/x) \\gt P(C_j/x)$ from the Bayes theorem:$\\\\[10pt]$\n",
    "\n",
    "$$d_k(x) = P(C_k/\\mathbf{x}) = \\frac{p(\\mathbf{x}/C_k)P(C_k)}{P(\\mathbf{x})}\\\\[5pt]$$\n",
    "\n",
    "This is, a way to compute the posterior probability $P(C_k/\\mathbf{x})$ of a feature vector $\\mathbf{x}$ for each class. Anyways, we can simplify this function:$\\\\[10pt]$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&d_k(x) &= P(C_k/\\mathbf{x}) = \\frac{p(\\mathbf{x}/C_k)P(C_k)}{P(\\mathbf{x})}\\\\\n",
    "\\xrightarrow{P(\\mathbf{x})\\text{ is a constant value }\\forall k} \\hspace{0.2cm}&d_k(x) &= p(\\mathbf{x}/C_k)P(C_k)\\\\[7pt]\n",
    "\\xrightarrow{\\text{ ln doesn't alter maximum }} \\hspace{0.2cm} &d_k(x) &= lnp(\\mathbf{x}/C_k) + lnP(C_k)\\\\[7pt]\n",
    "\\xrightarrow{\\text{If } P(C_k) = P(C_j)\\; \\forall \\ j,k }\\hspace{0.2cm} &d_k(x) &= lnp(\\mathbf{x}/C_k)\n",
    "\\end{eqnarray}\n",
    "\\\\[8pt]$$\n",
    "\n",
    "This is called **maximum log-likelihood estimation**.\n",
    "\n",
    "### 7.2.2 Naive Bayes classifier for normal distribution\n",
    "\n",
    "We can suppose that features follow a normal distribution when applying a Bayesian classifier.$\\\\[5pt]$\n",
    "\n",
    "$$p(\\mathbf{x}/C_i) = \\large{\\frac{1}{(2\\pi)^{n/2}|\\Sigma^i|^{1/2}}}e^{-(\\mathbf{x}-\\mu^i)^T\\Sigma_i^{-1}\\mathbf{x}-\\mu^i)}\\\\[5pt]$$\n",
    "\n",
    "In that way, the discriminant function:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "d_k(\\mathbf{x}) &=& ln\\ P(C_k) + ln\\ p(\\mathbf{x}/C_k) = ln\\ P(C_k) + ln\\ \\frac{1}{(2\\pi)^{n/2}|\\Sigma^i|^{1/2}}e^{\\color{red}{-(\\mathbf{x}-\\mu^i)^T\\Sigma_i^{-1}\\mathbf{x}-\\mu^i)}}\\\\[7pt]\n",
    "    &=& ln\\ P(C_k) - ln\\ 2\\pi^{n/2} - ln\\ |\\Sigma^k|^{1/2} - \\frac{1}{2} \\color{red}{D_k^2(x)}\\\\[7pt]\n",
    "    &=& ln\\ P(C_k) - \\frac{1}{2}\\left[n ln\\ 2\\pi +  ln\\ |\\Sigma^k| + D_k^2(x)\\right] \\xrightarrow{\\text{cte can be removed}} ln\\ P(C_k) - \\frac{1}{2}\\left[ln\\ |\\Sigma^k| + D_k^2(x)\\right]\\\\[7pt]\n",
    "\\end{eqnarray}\n",
    "\\\\[8pt]$$\n",
    "\n",
    "We can see that the **discriminant function is quadratic**:\n",
    "\n",
    "$$d_k(\\mathbf{x}) = ln\\ P(C_k) - \\underbrace{ \\frac{1}{2} ln\\ |\\Sigma^k| - \\left[(\\mathbf{x-\\mu})^T (\\Sigma^k)^{-1} (\\mathbf{x-\\mu}) \\right]}_{\\text{Independent term}} + \\mathbf{x}^T (\\underbrace{\\Sigma^k)^{-1}\\mathbf{\\mu}^k}_{\\text{linear term}} - \\frac{1}{2} \\mathbf{x}^T \\underbrace{(\\Sigma^k)^{-1}}_{\\text{Quadratic term}}\\mathbf{x}$$\n",
    "\n",
    "Visually, **division boundaries are parabolas**:\n",
    "\n",
    "<img src=\"./images/visual_normal_bayes.png\" width=\"600\">$\\\\[3pt]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1a</i></b></span>**\n",
    "\n",
    "Now, we are going to implement this naive bayes classifier for normal distributions using the Hu moments computed in the previous exercise.\n",
    "\n",
    "**Training the classifier**\n",
    "\n",
    "The first step is computing the weight matrix of the discriminant function. In this case, it depends on the **means** ($\\bf{\\mu}$) and **covariance matrix** ($\\Sigma$) of the training data.\n",
    "\n",
    "In the previous exercise we proved that out problem can be solved using only the first and second Hu moments. **Your first task** is to load (using [np.load](https://numpy.org/doc/stable/reference/generated/numpy.load.html?highlight=load#numpy.load)) the firsts two Hu moments of each class' data, which was computed in previous notebook. Then, **compute the means** (or centroid) and **covariance matrix** for each class.\n",
    "\n",
    "*We can compute the covariance matrix of a set of points using [np.cov](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first 2 Hu moments of each class \n",
    "\n",
    "\n",
    "# Compute covariance matrices\n",
    "\n",
    "\n",
    "# Compute means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1b</i></b></span>**\n",
    "\n",
    "Your **next task** is to develop a method `discriminant_function`, that computes $d_k(x)$. The inputs have to be:\n",
    "\n",
    "- `features`: feature vector of dimension n\n",
    "- `mu`: mean vector of the class k of which is being computed the probability\n",
    "- `cov`: covariance matrix with shape (n,n) of the class k\n",
    "- `prior`: prior probability of class k\n",
    "\n",
    "The method should evaluate (then return) the discriminant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_function(features, mu, cov, prior):\n",
    "    \"\"\" Evaluates the discriminant function d(x)\n",
    "    \n",
    "        Args:\n",
    "            features: feature vector of dimension n\n",
    "            mu: mean vector of the class of which is being computed the probability\n",
    "            cov: covariance matrix with shape (n,n) of the class\n",
    "            prior: prior probability of class k\n",
    "\n",
    "        Returns:\n",
    "            dx: result of discriminant function\n",
    "    \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1c</i></b></span>**\n",
    "\n",
    "**Testing the classifier**\n",
    "\n",
    "For testing if the classifier works, we are going to classify some new images and check if the classifier is correct or not. *Note that the discriminant function is the logarithm of a probability, not a probability itself (values can be positive and negatives, but the result of max function is the same)*.\n",
    "\n",
    "For this, evaluate the images `train_circle.png`,`train_square.png` and `train_triangle.png` using the discriminant function with each class and choosing the maximum one.\n",
    "\n",
    "**What to do?** Compute the Hu moments of each training image and apply the classifier to detect their class (using the previous method). Try to construct a method that classify any image in order to save code.\n",
    "\n",
    "*We assume that there is no prior information of any class $P(C_i) = P(C_j)\\; \\forall i,j$. This is because a priori, we see the same number of circle, square and triangle shaped road signs when driving.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_moments(region):\n",
    "    \"\"\" Compute moments of the external contour in a binary image.   \n",
    "    \n",
    "        Args:\n",
    "            region: Binary image\n",
    "                    \n",
    "        Returns: \n",
    "            moments: dictionary containing all moments of the region\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Get external contour\n",
    "    \n",
    "    \n",
    "    # Compute moments\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(sign_image):\n",
    "    \"\"\" Classify a traffic sign image by its shape using a bayesian classifier   \n",
    "    \n",
    "        Args:\n",
    "            sign_image: Binarized image\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Compute Hu moments\n",
    "    \n",
    "    \n",
    "    # Classify circle test image\n",
    "    \n",
    "    \n",
    "    # Search the maximum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "test_circle = cv2.imread(images_path + \"test_circle.png\", 0)\n",
    "test_triangle = cv2.imread(images_path + \"test_triangle.png\", 0)\n",
    "test_square = cv2.imread(images_path + \"test_square.png\", 0)\n",
    "\n",
    "# Classify them\n",
    "print(\"Circle: \")\n",
    "classify_image(test_circle)\n",
    "print(\"Triangle: \")\n",
    "classify_image(test_triangle)\n",
    "print(\"Square: \")\n",
    "classify_image(test_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see how this classifier divide the feature space showing the covariance ellipses. **You have to modify the names of the variables** on this code to make it works, showing the covariance ellipces of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([0.14, 0.2, -0.01, 0.059])\n",
    "\n",
    "# Plot hu moments\n",
    "plt.plot(train_triangles[0,:],train_triangles[1,:],'go')\n",
    "plt.plot(train_circles[0,:],train_circles[1,:],'ro')\n",
    "plt.plot(train_squares[0,:],train_squares[1,:],'bo')\n",
    "\n",
    "# Plot ellipses representing covariance matrices\n",
    "PlotEllipse(fig, ax, np.vstack(mean_triangles), cov_triangles, 5, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(mean_squares), cov_squares, 5, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(mean_circles), cov_circles, 5, color='black')\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2</i></b></span>**\n",
    "\n",
    "**Simplification**\n",
    "\n",
    "We can simplify our classifier using Euclidean distance instead of Mahalanobis one. This can be achieved using isotropic covariance matrices:\n",
    "\n",
    "$$\\Sigma^k = \\Sigma = \\sigma^2 \\cdot I = \\sigma^2 \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "In this way, decision boundaries are lines, and covariances are spherical. This is called a **natural classifier**:$\\\\[5pt]$\n",
    "\n",
    "<img src=\"./images/natural_classifier.png\" width=\"500\">$\\\\[3pt]$\n",
    "\n",
    "The discriminant function can be simplified, and the cuadratic term disappear:$\\\\[6pt]$\n",
    "\n",
    "$$d_k(x) = -(\\mathbf{x-\\mu^k})^T(\\mathbf{x-\\mu^k}) = -||\\mathbf{x-\\mu^k}||^2\\\\[6pt]$$\n",
    "\n",
    "**What to do?** Repeat the previous exercise using isotropic covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_function_isotropic(features, mu):\n",
    "    \"\"\" Evaluates the discriminant function of a naive Bayes clasifier using isotropic covariances\n",
    "    \n",
    "        Args:\n",
    "            features: feature vector of dimension n\n",
    "            mu: mean vector of the class of which is being computed the probability\n",
    "\n",
    "        Returns:\n",
    "            dx: result of discriminant function\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_isotropic(sign_image):\n",
    "    \"\"\" Classify a traffic sign image by its shape using a bayesian classifier   \n",
    "    \n",
    "        Args:\n",
    "            sign_image: Binarized image\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Compute Hu moments\n",
    "    \n",
    "    \n",
    "    # Classify circle test image\n",
    "    \n",
    "    \n",
    "    # Search the maximum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "test_circle = cv2.imread(images_path + \"test_circle.png\", 0)\n",
    "test_triangle = cv2.imread(images_path + \"test_triangle.png\", 0)\n",
    "test_square = cv2.imread(images_path + \"test_square.png\", 0)\n",
    "\n",
    "# Classify them\n",
    "print(\"Circle: \")\n",
    "classify_image_isotropic(test_circle)\n",
    "print(\"Triangle: \")\n",
    "classify_image_isotropic(test_triangle)\n",
    "print(\"Square: \")\n",
    "classify_image_isotropic(test_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([0.14, 0.2, -0.01, 0.059])\n",
    "\n",
    "# Plot hu moments\n",
    "plt.plot(train_triangles[0,:],train_triangles[1,:],'go')\n",
    "plt.plot(train_circles[0,:],train_circles[1,:],'ro')\n",
    "plt.plot(train_squares[0,:],train_squares[1,:],'bo')\n",
    "\n",
    "# Plot ellipses representing covariance matrices\n",
    "PlotEllipse(fig, ax, np.vstack(mean_triangles), np.eye(2), 0.003, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(mean_squares), np.eye(2), 0.003, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(mean_circles), np.eye(2), 0.003, color='black')\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Now, **answer the following questions**:\n",
    "\n",
    "- What are the pros and cons of using isotropic covariances?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- In what type of problems could isotropic matrices be used?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Awesome! You know now how to make a classifier of **previously segmented objects**. Note that for more complex shapes, you can use the **7 Hu moments instead of the two that we used** for visualization purposes and simplicity of the problem.\n",
    "\n",
    "In this notebook you have learn to:\n",
    "\n",
    "- construct a naive Bayes classifier and apply it to a real problem where variables follow a normal distribuion\n",
    "- construct a simplified classifier where isotropic covariances are assumed\n",
    "- improve a classifier (if needed) using rejection regions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
